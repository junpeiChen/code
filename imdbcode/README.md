# IMDB ç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†æ

æœ¬é¡¹ç›®ä½¿ç”¨å¤šç§æœºå™¨å­¦ä¹ æ¨¡å‹å¯¹ IMDB ç”µå½±è¯„è®ºè¿›è¡Œæƒ…æ„Ÿåˆ†æï¼Œé¢„æµ‹è¯„è®ºçš„æƒ…æ„Ÿå€¾å‘ï¼ˆæ­£é¢/è´Ÿé¢ï¼‰ã€‚
## ğŸ“Š é¡¹ç›®æ¦‚è¿°

- **ä»»åŠ¡ç±»å‹**ï¼šäºŒåˆ†ç±»æƒ…æ„Ÿåˆ†æ
- **æ•°æ®æ¥æº**ï¼šIMDB ç”µå½±è¯„è®ºæ•°æ®é›†
- **ç›®æ ‡**ï¼šæ ¹æ®ç”µå½±è¯„è®ºå†…å®¹é¢„æµ‹æƒ…æ„Ÿå€¾å‘ï¼ˆ0=è´Ÿé¢ï¼Œ1=æ­£é¢ï¼‰
- ## ğŸ“ æ•°æ®é›†

### è®­ç»ƒæ•°æ® (`labeledTrainData.tsv`)
- 25,000 æ¡å¸¦æ ‡ç­¾çš„å½±è¯„
- åŒ…å«ä»¥ä¸‹åˆ—ï¼š
  - `id`: è¯„è®ºå”¯ä¸€æ ‡è¯†
  - `sentiment`: æƒ…æ„Ÿæ ‡ç­¾ (0=è´Ÿé¢, 1=æ­£é¢)
  - `review`: è¯„è®ºæ–‡æœ¬

### æµ‹è¯•æ•°æ® (`testData.tsv`)
- 25,000 æ¡æœªæ ‡è®°çš„å½±è¯„
- åŒ…å«ä»¥ä¸‹åˆ—ï¼š
  - `id`: è¯„è®ºå”¯ä¸€æ ‡è¯†
  - `review`: è¯„è®ºæ–‡æœ¬

## ğŸ¤– ä½¿ç”¨çš„æ¨¡å‹

æœ¬é¡¹ç›®å®ç°äº†å¤šç§æ¨¡å‹è¿›è¡Œå¯¹æ¯”åˆ†æï¼š
1. **attention_lstm**
2. **bert_native**
3. **bert_scratch**
4. **bert_trainer**
5. **capsule_lstm**
6. **cnn**
7. **cnnlstm**
8. **distilbert_native**
9. **distilbert_trainer**
10. **gru**
11. **lstm**
12. **roberta_trainer**
13. **transformer**

### ç¯å¢ƒè¦æ±‚

```bash
pip install transformers
pip install datasets
pip install pandas
pip install numpy
pip install scikit-learn
pip install evaluate
pip intsall torch
```
## ğŸ“ˆ æ¨¡å‹æ€§èƒ½å¯¹æ¯”
### å‡†ç¡®ç‡å¯¹æ¯”è¡¨

| æ¨¡å‹ | æµ‹è¯•é›†æœ€é«˜å‡†ç¡®ç‡ | Epochæ¬¡æ•° | Kaggleåˆ†æ•° | å¤‡æ³¨ |
|------|--------------|-----------|------------|------|
| attention_lstm | 0.82 | 10 | 0.81 | æ³¨æ„åŠ›æœºåˆ¶+LSTM |
| bert_native | 0.92 | 3 | 0.87 | åŸç”ŸBERTå®ç° |
| bert_scratch | 0.93 | 3 | 0.93 | ä»å¤´è®­ç»ƒçš„BERT |
| bert_trainer | 0.93 | 3 | 0.94 | ä½¿ç”¨Trainerçš„BERT |
| capsule_lstm | 0.90 | 7 | 0.87 | èƒ¶å›Šç½‘ç»œ+LSTM |
| cnn | 0.87 | 10 | 0.86 | å·ç§¯ç¥ç»ç½‘ç»œ |
| cnnlstm | 0.86 | 10 | 0.85 | CNN+LSTMæ··åˆæ¨¡å‹ |
| distilbert_native | 0.91 | 3 | 0.92 | åŸç”ŸDistilBERT |
| distilbert_trainer | 0.93 | 3 | 0.93 | ä½¿ç”¨Trainerçš„DistilBERT |
| gru | 0.84 | 10 | 0.84 | é—¨æ§å¾ªç¯å•å…ƒ |
| lstm | 0.89 | 10 | 0.88 | é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ |
| roberta_trainer | 0.94 | 1 | 0.95 | ä½¿ç”¨Trainerçš„RoBERTa |
| transformer | 0.80 | 10 | 0.79 | Transformerç¼–ç å™¨ |
| bert-rdrop | 0.91 | 3 | 0.91 | - |
| bert-scl-trainer | 0.93 | 3 | 0.93 | - |
| modernbert-unsloth | 0.95 | 3 | 0.95 | - |
## ğŸ“Š ç»“æœåˆ†æ
åŸºäºæä¾›çš„å®éªŒç»“æœï¼Œæ‰€æœ‰æ¨¡å‹åœ¨IMDBç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œæµ‹è¯•é›†å‡†ç¡®ç‡åˆ†å¸ƒåœ¨80%-94%ä¹‹é—´ï¼Œä½“ç°äº†ç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šçš„å¼ºå¤§èƒ½åŠ›ã€‚
1. é¢„è®­ç»ƒæ¨¡å‹çš„ç»å¯¹ä¼˜åŠ¿
2. åˆé€‚çš„è¶…å‚æ•°è°ƒä¼˜å¯¹ä¼ ç»Ÿæ¨¡å‹è‡³å…³é‡è¦
3. å¯èƒ½å—ç›Šäºæ›´å¥½çš„åˆå§‹åŒ–æˆ–æ­£åˆ™åŒ–ç­–ç•¥
## æ¨¡å‹æ€§èƒ½å¯¹æ¯”

### 1. DeBERTa-base å¾®è°ƒç»“æœï¼ˆç›‘ç£å­¦ä¹ ï¼‰

| è®­ç»ƒæ ·æœ¬æ•° | éªŒè¯é›†å‡†ç¡®ç‡ |
|-----------|-------------|
| 16        | 53.78%      |
| 64        | 66.06%      |
| 256       | 86.47%      |
| 1024      | 91.40%      |
| 67349ï¼ˆå…¨é‡ï¼‰ | 93.69%      |

### 2. å¤§è¯­è¨€æ¨¡å‹å­¦ä¹ ç»“æœ

| æ¨¡å‹ | å‚æ•°é‡ | å‡†ç¡®ç‡ |
|------|--------|--------|
| TinyLlama-1.1B | 1.1B | 33.33% |
| Qwen-0.5B | 0.5B | 46.50% |
| Phi-2-2.7B | 2.7B | 64.95% |
| Mistral-7B | 7B | 87.31% |
| Gemma-2B | 2B | 91.50% |

*æµ‹è¯•æ ·æœ¬é‡ï¼š200*

### Qwen2.5-0.5B ç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†ç±»é¡¹ç›®
æœ¬é¡¹ç›®ä½¿ç”¨ Qwen2.5-0.5B-Instruct æ¨¡å‹ï¼Œé€šè¿‡å…ˆè¿›çš„æç¤ºè¯å·¥ç¨‹å’Œå¾®è°ƒæŠ€æœ¯ï¼Œåœ¨IMDbç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ä¸Šå®ç°äº† 89% çš„éªŒè¯å‡†ç¡®ç‡ã€‚
| æ¨¡å‹ | å‚æ•°é‡ | å‡†ç¡®ç‡ |
|------|--------|--------|
| Qwen-0.5B | 0.5B | 89.43% |
