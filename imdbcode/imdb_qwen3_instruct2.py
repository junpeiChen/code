import os

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

import sys
import logging
import pandas as pd
import numpy as np
import torch
import re
from datasets import Dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from transformers import DataCollatorForLanguageModeling

from unsloth import FastLanguageModel
from transformers import TrainingArguments
from trl import SFTTrainer

# --- æ—¥å¿—è®¾ç½® ---
program = os.path.basename(sys.argv[0])
logger = logging.getLogger(program)
logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')
logging.root.setLevel(level=logging.INFO)

# --- å¤æ‚æç¤ºè¯å·¥ç¨‹ ---
# è®­ç»ƒæç¤ºè¯ - åŒ…å«è¯¦ç»†çš„ä»»åŠ¡æè¿°å’Œç¤ºä¾‹
train_prompt = """You are an expert sentiment analysis system specialized in movie reviews. 

**TASK**: 
Classify the sentiment of the movie review as either POSITIVE or NEGATIVE.

**GUIDELINES**:
- POSITIVE: Reviews that express satisfaction, enjoyment, or recommendation
- NEGATIVE: Reviews that express disappointment, criticism, or discouragement

**EXAMPLES**:
- "This movie was absolutely fantastic! Great acting and plot." â†’ POSITIVE
- "Terrible movie, wasted my time and money." â†’ NEGATIVE
- "The cinematography was beautiful but the story was weak." â†’ NEGATIVE
- "Despite some flaws, I thoroughly enjoyed this film." â†’ POSITIVE

**REVIEW TO CLASSIFY**:
{}

**SENTIMENT CLASSIFICATION**:
{}"""

# æ¨ç†æç¤ºè¯ - ä¸è®­ç»ƒæ—¶ç•¥æœ‰ä¸åŒï¼Œä¸åŒ…å«ç­”æ¡ˆ
inference_prompt = """You are an expert sentiment analysis system specialized in movie reviews. 

**TASK**: 
Classify the sentiment of the movie review as either POSITIVE or NEGATIVE.

**GUIDELINES**:
- POSITIVE: Reviews that express satisfaction, enjoyment, or recommendation
- NEGATIVE: Reviews that express disappointment, criticism, or discouragement

**EXAMPLES**:
- "This movie was absolutely fantastic! Great acting and plot." â†’ POSITIVE
- "Terrible movie, wasted my time and money." â†’ NEGATIVE
- "The cinematography was beautiful but the story was weak." â†’ NEGATIVE
- "Despite some flaws, I thoroughly enjoyed this film." â†’ POSITIVE

**REVIEW TO CLASSIFY**:
{}

**SENTIMENT CLASSIFICATION**:
"""


# --- æ–‡æœ¬é¢„å¤„ç†å‡½æ•° ---
def preprocess_text(text):
    """æ¸…ç†å’Œé¢„å¤„ç†æ–‡æœ¬"""
    if pd.isna(text):
        return ""

    # ç§»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<br\s*/?>', ' ', text)
    text = re.sub(r'<[^>]+>', ' ', text)

    # ç§»é™¤å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\\[ntr]', ' ', text)

    # é™åˆ¶é•¿åº¦
    text = text.strip()[:800]

    return text


# --- æ•°æ®æ ¼å¼åŒ–å‡½æ•° ---
def formatting_prompts_func(examples):
    inputs = examples["text"]
    labels = examples["label"]
    outputs_text = []

    for input_text, label in zip(inputs, labels):
        # é¢„å¤„ç†æ–‡æœ¬
        clean_text = preprocess_text(input_text)
        label_text = "POSITIVE" if label == 1 else "NEGATIVE"
        text = train_prompt.format(clean_text, label_text) + tokenizer.eos_token
        outputs_text.append(text)

    return {"text": outputs_text}


# --- é«˜çº§è§£æå‡½æ•° ---
def parse_model_output(generated_text):
    """é«˜çº§è§£ææ¨¡å‹è¾“å‡ºï¼Œä½¿ç”¨å¤šç§ç­–ç•¥"""
    generated_text_lower = generated_text.lower()

    # ç­–ç•¥1: æŸ¥æ‰¾åˆ†ç±»æ ‡è®°åçš„å†…å®¹
    classification_markers = ["sentiment classification:", "classification:", "sentiment:"]

    for marker in classification_markers:
        if marker in generated_text_lower:
            parts = generated_text_lower.split(marker, 1)
            if len(parts) > 1:
                response_part = parts[1].strip()
                # æå–ç¬¬ä¸€ä¸ªå•è¯æˆ–å‰å‡ ä¸ªå•è¯
                first_word = response_part.split()[0] if response_part.split() else ""

                if first_word in ["positive", "pos"]:
                    return 1
                elif first_word in ["negative", "neg"]:
                    return 0

    # ç­–ç•¥2: ç›´æ¥æœç´¢å…³é”®è¯ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰
    positive_indicators = [
        "positive", "pos", "good", "great", "excellent", "amazing",
        "wonderful", "fantastic", "brilliant", "love", "liked", "enjoyed",
        "recommend", "awesome", "outstanding"
    ]

    negative_indicators = [
        "negative", "neg", "bad", "terrible", "awful", "horrible",
        "boring", "waste", "disappointing", "hate", "dislike", "poor",
        "worst", "weak", "terrible"
    ]

    # æ£€æŸ¥æ•´ä¸ªç”Ÿæˆæ–‡æœ¬ä¸­çš„å…³é”®è¯
    positive_count = sum(1 for word in positive_indicators if word in generated_text_lower)
    negative_count = sum(1 for word in negative_indicators if word in generated_text_lower)

    if positive_count > negative_count:
        return 1
    elif negative_count > positive_count:
        return 0

    # ç­–ç•¥3: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ¨¡å¼
    positive_patterns = [
        r'\bpositive\b', r'\bpos\b', r'classify.*positive', r'sentiment.*positive'
    ]
    negative_patterns = [
        r'\bnegative\b', r'\bneg\b', r'classify.*negative', r'sentiment.*negative'
    ]

    for pattern in positive_patterns:
        if re.search(pattern, generated_text_lower):
            return 1

    for pattern in negative_patterns:
        if re.search(pattern, generated_text_lower):
            return 0

    # ç­–ç•¥4: åŸºäºæƒ…æ„Ÿè¯æ±‡çš„å¯å‘å¼åˆ†æ
    strong_positive_words = ["love", "amazing", "fantastic", "brilliant", "masterpiece"]
    strong_negative_words = ["hate", "terrible", "awful", "horrible", "worst"]

    for word in strong_positive_words:
        if word in generated_text_lower:
            return 1

    for word in strong_negative_words:
        if word in generated_text_lower:
            return 0

    # é»˜è®¤è¿”å›è´Ÿå‘ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
    return 0


# --- ä¸»æ‰§è¡Œç¨‹åº ---
if __name__ == '__main__':
    logger.info(r"running %s" % ''.join(sys.argv))

    # --- 1. åŠ è½½å’Œå‡†å¤‡æ•°æ® ---
    logger.info("Loading data...")
    try:
        train_df = pd.read_csv(r"/root/autodl-tmp/labeledTrainData.tsv", header=0, delimiter="\t", quoting=3)
        test_df = pd.read_csv(r"/root/autodl-tmp/testData.tsv", header=0, delimiter="\t", quoting=3)
    except Exception as e:
        logger.error(f"Error loading data: {e}")
        sys.exit(1)

    # æ•°æ®åˆ†æå’Œç»Ÿè®¡
    logger.info(f"Training data size: {len(train_df)}")
    logger.info(f"Sentiment distribution: {train_df['sentiment'].value_counts().to_dict()}")

    # åˆ†æè¯„è®ºé•¿åº¦
    train_df['review_length'] = train_df['review'].str.len()
    logger.info(f"Review length stats - Mean: {train_df['review_length'].mean():.2f}, "
                f"Max: {train_df['review_length'].max()}, Min: {train_df['review_length'].min()}")

    # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†
    train_df, val_df = train_test_split(
        train_df,
        test_size=0.15,
        random_state=3407,
        stratify=train_df['sentiment']
    )

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = Dataset.from_dict({'label': train_df["sentiment"], 'text': train_df['review']})
    val_dataset = Dataset.from_dict({'label': val_df["sentiment"], 'text': val_df['review']})
    test_dataset = Dataset.from_dict({"text": test_df['review']})

    # --- 2. åŠ è½½æ¨¡å‹å’Œ Tokenizer ---
    logger.info("Loading Qwen model...")
    model_name = r"/root/autodl-tmp/Qwen2.5-0.5B-Instruct"

    try:
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=model_name,
            max_seq_length=1024,
            dtype=None,
            load_in_4bit=True,
        )
    except Exception as e:
        logger.error(f"Error loading model: {e}")
        sys.exit(1)

    # --- 3. PEFT (LoRA) è®¾ç½® ---
    logger.info("Setting up PEFT...")
    model = FastLanguageModel.get_peft_model(
        model,
        r=32,
        lora_alpha=64,
        lora_dropout=0.1,
        bias="none",
        random_state=3407,
        use_gradient_checkpointing="unsloth",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                        "gate_proj", "up_proj", "down_proj"],
    )

    # --- 4. æ ¼å¼åŒ–æ•°æ®é›† ---
    logger.info("Formatting datasets with advanced prompts...")
    train_dataset = train_dataset.map(formatting_prompts_func, batched=True)
    val_dataset = val_dataset.map(formatting_prompts_func, batched=True)

    # --- 5. è®­ç»ƒå‚æ•° ---
    logger.info("Setting up Training Arguments...")
    training_args = TrainingArguments(
        output_dir="./qwen_advanced_prompt_output",
        overwrite_output_dir=True,
        per_device_train_batch_size=4,  # è¾ƒå°çš„æ‰¹æ¬¡å¤§å°ä»¥é€‚åº”æ›´é•¿çš„åºåˆ—
        per_device_eval_batch_size=8,
        gradient_accumulation_steps=4,
        warmup_ratio=0.1,  # ä½¿ç”¨æ¯”ä¾‹è€Œéå›ºå®šæ­¥æ•°
        num_train_epochs=4,
        learning_rate=3e-5,  # ç¨ä½çš„å­¦ä¹ ç‡
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=25,
        save_steps=200,
        eval_steps=200,
        evaluation_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        report_to=[],
        seed=3407,
        data_seed=3407,
        group_by_length=True,  # æŒ‰é•¿åº¦åˆ†ç»„æé«˜æ•ˆç‡
        dataloader_pin_memory=False,
    )

    # --- 6. åˆå§‹åŒ– SFTTrainer ---
    logger.info("Initializing SFTTrainer with advanced prompts...")

    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        dataset_text_field="text",
        max_seq_length=1024,
        dataset_num_proc=2,
        packing=False,
        args=training_args,
    )

    # --- 7. è®­ç»ƒ ---
    logger.info("Starting training with advanced prompts...")

    # è®­ç»ƒå‰è¯„ä¼°
    logger.info("Pre-training evaluation...")
    pre_train_eval = trainer.evaluate()
    logger.info(f"Pre-training evaluation results: {pre_train_eval}")

    # è®­ç»ƒæ¨¡å‹
    train_result = trainer.train()

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    trainer.save_model()
    logger.info("Model saved successfully.")

    # è®­ç»ƒåè¯„ä¼°
    logger.info("Post-training evaluation...")
    post_train_eval = trainer.evaluate()
    logger.info(f"Post-training evaluation results: {post_train_eval}")

    # è®°å½•è®­ç»ƒæŒ‡æ ‡
    metrics = train_result.metrics
    logger.info(f"Training metrics: {metrics}")

    # --- 8. åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°å‡†ç¡®ç‡ ---
    logger.info("Evaluating on validation set with advanced parsing...")
    FastLanguageModel.for_inference(model)

    val_texts = val_df['review'].tolist()
    val_labels = val_df['sentiment'].tolist()
    val_predictions = []

    # ä½¿ç”¨æ‰¹æ¬¡æ¨ç†
    batch_size = 8  # è¾ƒå°çš„æ‰¹æ¬¡å¤§å°ä»¥é€‚åº”æ›´é•¿çš„æç¤ºè¯

    for i in range(0, len(val_texts), batch_size):
        batch_texts = val_texts[i:i + batch_size]
        batch_prompts = [inference_prompt.format(preprocess_text(text)) for text in batch_texts]

        inputs = tokenizer(batch_prompts, return_tensors="pt", padding=True, truncation=True, max_length=1024).to(
            "cuda")

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=20,  # å¢åŠ ç”Ÿæˆé•¿åº¦ä»¥å®¹çº³æ›´å¤æ‚çš„å“åº”
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.eos_token_id,
                do_sample=False,
                temperature=0.1,
                repetition_penalty=1.1,
            )

        generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)

        for j, generated_text in enumerate(generated_texts):
            prediction = parse_model_output(generated_text)
            val_predictions.append(prediction)

            # è®°å½•ä¸€äº›ç¤ºä¾‹ç”¨äºè°ƒè¯•
            if i == 0 and j < 3:
                logger.info(f"Example {j + 1}:")
                logger.info(f"Prompt: {batch_prompts[j][:200]}...")
                logger.info(f"Generated: {generated_text}")
                logger.info(f"Predicted: {prediction}, Actual: {val_labels[i + j]}")

    # è®¡ç®—éªŒè¯é›†å‡†ç¡®ç‡
    val_accuracy = accuracy_score(val_labels, val_predictions)
    logger.info(f"Validation Accuracy: {val_accuracy:.4f}")

    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    logger.info(f"Classification Report:\n{classification_report(val_labels, val_predictions)}")

    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(val_labels, val_predictions)
    logger.info(f"Confusion Matrix:\n{cm}")

    # --- 9. é”™è¯¯åˆ†æ ---
    logger.info("Performing error analysis...")
    errors = []
    for i, (true, pred) in enumerate(zip(val_labels, val_predictions)):
        if true != pred:
            errors.append({
                'text': val_texts[i][:500] + "..." if len(val_texts[i]) > 500 else val_texts[i],
                'true': true,
                'predicted': pred
            })

    logger.info(f"Number of errors: {len(errors)}")
    if errors:
        logger.info("Sample errors:")
        for i, error in enumerate(errors[:3]):
            logger.info(f"Error {i + 1}: True={error['true']}, Predicted={error['predicted']}")
            logger.info(f"Text: {error['text']}")

    # --- 10. åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹ ---
    logger.info("Starting test set inference with advanced prompts...")

    test_texts = test_df['review'].tolist()
    test_ids = test_df['id'].tolist()
    test_predictions = []

    for i in range(0, len(test_texts), batch_size):
        batch_texts = test_texts[i:i + batch_size]
        batch_prompts = [inference_prompt.format(preprocess_text(text)) for text in batch_texts]

        inputs = tokenizer(batch_prompts, return_tensors="pt", padding=True, truncation=True, max_length=1024).to(
            "cuda")

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=20,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.eos_token_id,
                do_sample=False,
                temperature=0.1,
                repetition_penalty=1.1,
            )

        generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)

        for generated_text in generated_texts:
            prediction = parse_model_output(generated_text)
            test_predictions.append(prediction)

        if (i // batch_size) % 20 == 0:
            logger.info(f"Processed {i + len(batch_texts)}/{len(test_texts)} test samples")

    # --- 11. ä¿å­˜ç»“æœ ---
    logger.info("Saving results...")
    results_dir = "./advanced_prompt_results"
    os.makedirs(results_dir, exist_ok=True)

    # ä¿å­˜æµ‹è¯•é›†é¢„æµ‹
    result_output = pd.DataFrame(data={"id": test_ids, "sentiment": test_predictions})
    result_path = os.path.join(results_dir, "qwen_advanced_prompt_predictions.csv")
    result_output.to_csv(result_path, index=False, quoting=3)
    logger.info(f'Test predictions saved to: {result_path}')

    # ä¿å­˜éªŒè¯é›†ç»“æœç”¨äºåˆ†æ
    val_results = pd.DataFrame({
        "review": val_texts,
        "true_label": val_labels,
        "predicted_label": val_predictions
    })
    val_results_path = os.path.join(results_dir, "advanced_validation_results.csv")
    val_results.to_csv(val_results_path, index=False)
    logger.info(f'Validation results saved to: {val_results_path}')

    # ä¿å­˜é”™è¯¯åˆ†æ
    if errors:
        errors_df = pd.DataFrame(errors)
        errors_path = os.path.join(results_dir, "error_analysis.csv")
        errors_df.to_csv(errors_path, index=False)
        logger.info(f'Error analysis saved to: {errors_path}')

    # æœ€ç»ˆç»Ÿè®¡
    logger.info("=== FINAL RESULTS ===")
    logger.info(f"Validation Accuracy: {val_accuracy:.4f}")
    logger.info(
        f"Test set predictions - Positive: {sum(test_predictions)}, Negative: {len(test_predictions) - sum(test_predictions)}")

    if val_accuracy >= 0.80:
        logger.info("ğŸ‰ SUCCESS: Model achieved target accuracy of 80% or higher!")
    elif val_accuracy >= 0.75:
        logger.info("âœ… GOOD: Model achieved good accuracy (75-80%).")
    else:
        logger.info("âš ï¸  NEEDS IMPROVEMENT: Model accuracy below 75%. Consider:")
        logger.info("   - Increasing training epochs")
        logger.info("   - Trying a larger model")
        logger.info("   - Further prompt engineering")
        logger.info("   - Hyperparameter tuning")